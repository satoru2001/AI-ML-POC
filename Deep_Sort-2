{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep_Sort-2","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyODARxct3slqjykm44iEW2s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bp3wHUH-sU1y","executionInfo":{"status":"ok","timestamp":1608980896598,"user_tz":-330,"elapsed":5199,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}},"outputId":"c4898a99-bbbc-4334-81f0-cc9c099bceed"},"source":["! git clone https://github.com/theAIGuysCode/yolov4-deepsort.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'yolov4-deepsort'...\n","remote: Enumerating objects: 137, done.\u001b[K\n","remote: Total 137 (delta 0), reused 0 (delta 0), pack-reused 137\u001b[K\n","Receiving objects: 100% (137/137), 76.82 MiB | 43.22 MiB/s, done.\n","Resolving deltas: 100% (40/40), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1jB91FPgsuuc"},"source":["%cd /content/yolov4-deepsort\r\n","!pip install -r requirements-gpu.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XEL0Wr9vs8fA","executionInfo":{"status":"ok","timestamp":1608980990568,"user_tz":-330,"elapsed":5118,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}},"outputId":"b643c5c8-559f-45ef-8d48-93bf70b56f62"},"source":["!gdown --id 1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT\n","To: /content/yolov4.weights\n","258MB [00:01, 155MB/s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6QnYFbKGuLIV","executionInfo":{"status":"ok","timestamp":1608981007515,"user_tz":-330,"elapsed":1125,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}}},"source":["%cp /content/yolov4.weights /content/yolov4-deepsort/data/"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lqEBC5eiuy9O","executionInfo":{"status":"ok","timestamp":1608990146765,"user_tz":-330,"elapsed":934,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}},"outputId":"6e9d2634-c6f9-404a-d772-dd6af0c67a47"},"source":["%cd /content/yolov4-deepsort"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/yolov4-deepsort\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xoY9aNc5udnG"},"source":["!python save_model.py --model yolov4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63QfpR6hukt4","executionInfo":{"status":"ok","timestamp":1608994024685,"user_tz":-330,"elapsed":994,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}},"outputId":"f1bb0481-33f3-4aae-95cc-0094434930c5"},"source":["import os\r\n","# comment out below line to enable tensorflow logging outputs\r\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n","import time\r\n","import tensorflow as tf\r\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n","if len(physical_devices) > 0:\r\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n","from absl import app, flags, logging\r\n","from absl.flags import FLAGS\r\n","import core.utils as utils\r\n","from core.yolov4 import filter_boxes\r\n","from tensorflow.python.saved_model import tag_constants\r\n","from core.config import cfg\r\n","from PIL import Image\r\n","import cv2\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","from tensorflow.compat.v1 import ConfigProto\r\n","from tensorflow.compat.v1 import InteractiveSession\r\n","# deep sort imports\r\n","from deep_sort import preprocessing, nn_matching\r\n","from deep_sort.detection import Detection\r\n","from deep_sort.tracker import Tracker\r\n","from tools import generate_detections as gdet\r\n","\r\n","max_cosine_distance = 0.2\r\n","nn_budget = None\r\n","nms_max_overlap = 1.0\r\n","    \r\n","    # initialize deep sort\r\n","model_filename = 'model_data/mars-small128.pb'\r\n","encoder = gdet.create_box_encoder(model_filename, batch_size=1)\r\n","# calculate cosine distance metric\r\n","metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\r\n","# initialize tracker\r\n","tracker = Tracker(metric)\r\n","\r\n","    # load configuration for object detector\r\n","\r\n","classes = 80\r\n","weights = './checkpoints/yolov4-416'\r\n","tiny = False\r\n","size = 416\r\n","\r\n","config = ConfigProto()\r\n","config.gpu_options.allow_growth = True\r\n","session = InteractiveSession(config=config)\r\n","STRIDES, ANCHORS, NUM_CLASS, XYSCALE = utils.load_config()\r\n","input_size = size"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hQQqPtpPy9i8","executionInfo":{"status":"ok","timestamp":1608990182296,"user_tz":-330,"elapsed":18916,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}}},"source":["saved_model_loaded = tf.saved_model.load(weights, tags=[tag_constants.SERVING])\r\n","infer = saved_model_loaded.signatures['serving_default']\r\n","iou = 0.5\r\n","score = 0.3"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"-YBrzYvT7t7O","executionInfo":{"status":"ok","timestamp":1608994068432,"user_tz":-330,"elapsed":951,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}}},"source":["iou = 0.6\r\n","score = 0.4"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OBYXKng36e6m","executionInfo":{"status":"ok","timestamp":1608995938699,"user_tz":-330,"elapsed":962,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}},"outputId":"1e4c329f-e42f-4b95-ff79-99b438680c09"},"source":["import os\r\n","# comment out below line to enable tensorflow logging outputs\r\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n","import time\r\n","import tensorflow as tf\r\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\r\n","if len(physical_devices) > 0:\r\n","    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n","from absl import app, flags, logging\r\n","from absl.flags import FLAGS\r\n","import core.utils as utils\r\n","from core.yolov4 import filter_boxes\r\n","from tensorflow.python.saved_model import tag_constants\r\n","from core.config import cfg\r\n","from PIL import Image\r\n","import cv2\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","from tensorflow.compat.v1 import ConfigProto\r\n","from tensorflow.compat.v1 import InteractiveSession\r\n","# deep sort imports\r\n","from deep_sort import preprocessing, nn_matching\r\n","from deep_sort.detection import Detection\r\n","from deep_sort.tracker import Tracker\r\n","from tools import generate_detections as gdet\r\n","\r\n","max_cosine_distance = 0.2\r\n","nn_budget = None\r\n","nms_max_overlap = 1.0\r\n","    \r\n","    # initialize deep sort\r\n","model_filename = 'model_data/mars-small128.pb'\r\n","encoder = gdet.create_box_encoder(model_filename, batch_size=1)\r\n","# calculate cosine distance metric\r\n","metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\r\n","# initialize tracker\r\n","tracker = Tracker(metric)\r\n","\r\n","    # load configuration for object detector\r\n","\r\n","classes = 80\r\n","weights = './checkpoints/yolov4-416'\r\n","tiny = False\r\n","size = 416\r\n","\r\n","config = ConfigProto()\r\n","config.gpu_options.allow_growth = True\r\n","session = InteractiveSession(config=config)\r\n","STRIDES, ANCHORS, NUM_CLASS, XYSCALE = utils.load_config()\r\n","input_size = size\r\n","from imutils.video import VideoStream\r\n","from imutils.video import FPS\r\n","import numpy as np\r\n","import argparse\r\n","import imutils\r\n","import time\r\n","import dlib\r\n","import cv2\r\n","\r\n","input_video = '/content/1006.mp4'\r\n","output_video = '/content/output_sample.avi'\r\n","writer = None\r\n","W = None\r\n","H = None\r\n","totalFrames = 0\r\n","tolerance = 140\r\n","out_of_frames = 0\r\n","\r\n","working = 1"],"execution_count":44,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QA0O4I-Q1yV8","executionInfo":{"status":"ok","timestamp":1608995993372,"user_tz":-330,"elapsed":54244,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}},"outputId":"2d654fdc-ddcd-4a89-f3ca-d2a9a18a8b8d"},"source":["print(\"[INFO] opening video file...\")\r\n","vs = cv2.VideoCapture(input_video)\r\n","\r\n","fps = FPS().start()\r\n","flag = 0\r\n","\r\n","pCentroid = (None,None)\r\n","\r\n","while True:\r\n","\r\n","  ret,frame = vs.read()\r\n","\r\n","  if ret==False or frame is None:\r\n","    print('hello')\r\n","    break\r\n","  \r\n","  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n","  image = Image.fromarray(frame)\r\n","\r\n","  frame_size = frame.shape[:2]\r\n","  image_data = cv2.resize(frame, (input_size, input_size))\r\n","  image_data = image_data / 255.\r\n","  image_data = image_data[np.newaxis, ...].astype(np.float32)\r\n","\r\n","  batch_data = tf.constant(image_data)\r\n","  pred_bbox = infer(batch_data)\r\n","  for key, value in pred_bbox.items():\r\n","    boxes = value[:, :, 0:4]\r\n","    pred_conf = value[:, :, 4:]\r\n","\r\n","  boxes, scores, classes, valid_detections = tf.image.combined_non_max_suppression(\r\n","            boxes=tf.reshape(boxes, (tf.shape(boxes)[0], -1, 1, 4)),\r\n","            scores=tf.reshape(\r\n","                pred_conf, (tf.shape(pred_conf)[0], -1, tf.shape(pred_conf)[-1])),\r\n","            max_output_size_per_class=50,\r\n","            max_total_size=50,\r\n","            iou_threshold=iou,\r\n","            score_threshold=score\r\n","        )\r\n","  num_objects = valid_detections.numpy()[0]\r\n","  bboxes = boxes.numpy()[0]\r\n","  bboxes = bboxes[0:int(num_objects)]\r\n","  scores = scores.numpy()[0]\r\n","  scores = scores[0:int(num_objects)]\r\n","  classes = classes.numpy()[0]\r\n","  classes = classes[0:int(num_objects)]\r\n","\r\n","  class_names = utils.read_class_names(cfg.YOLO.CLASSES)\r\n","  names = []\r\n","  for i in range(len(classes)):\r\n","    names.append(class_names[int(classes[i])])\r\n","  names = np.array(names)\r\n","\r\n","  # format bounding boxes from normalized ymin, xmin, ymax, xmax ---> xmin, ymin, width, height\r\n","  original_h, original_w, _ = frame.shape\r\n","  bboxes = utils.format_boxes(bboxes, original_h, original_w)\r\n","\r\n","  # store all predictions in one parameter for simplicity when calling functions\r\n","  pred_bbox = [bboxes, scores, classes, num_objects]\r\n","\r\n","  # encode yolo detections and feed to tracker\r\n","  features = encoder(frame, bboxes)\r\n","  detections = [Detection(bbox, score, class_name, feature) for bbox, score, class_name, feature in zip(bboxes, scores, names, features)]\r\n","\r\n","  #initialize color map\r\n","  cmap = plt.get_cmap('tab20b')\r\n","  colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\r\n","\r\n","  \r\n","  if W is None or H is None:\r\n","    (H, W) = frame.shape[:2]\r\n","        \r\n","  if writer is None:\r\n","    fourcc = cv2.VideoWriter_fourcc(*'XVID')\r\n","    writer = cv2.VideoWriter(output_video, fourcc, 20.,(W, H))\r\n","\r\n","  # run non-maxima suppresion\r\n","  boxs = np.array([d.tlwh for d in detections])\r\n","  scores = np.array([d.confidence for d in detections])\r\n","  classes = np.array([d.class_name for d in detections])\r\n","  indices = preprocessing.non_max_suppression(boxs, classes, nms_max_overlap, scores)\r\n","        \r\n","  detections = [detections[i] for i in indices]  \r\n","  \r\n","  if flag ==0:\r\n","    boxs = np.array([d.tlwh for d in detections])\r\n","    classes = np.array([d.class_name for d in detections])\r\n","\r\n","    index = -1\r\n","    area = 0\r\n","    for i,(box,classs) in enumerate(zip(boxs,classes)):\r\n","      #print(classs)\r\n","      if classs=='person':\r\n","        temp_area = abs(box[2])*abs(box[3])\r\n","        if temp_area>area:\r\n","          index = i\r\n","          area = temp_area\r\n","          \r\n","    if index>-1:\r\n","      tracker.predict()\r\n","      det = detections[index]\r\n","      det = np.expand_dims(det,0)\r\n","      tracker.update(det)\r\n","      flag = 1\r\n","\r\n","      \r\n","    text = \"Status : Waiting\"\r\n","    cv2.putText(frame, text, (10,((0 * 20) + 20)),cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\r\n","\r\n","    if writer is not None:\r\n","      frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\r\n","      writer.write(frame)\r\n","\r\n","    continue\r\n","\r\n","  tracker.predict()\r\n","  tracker.update(detections)\r\n","  \r\n","  present = 0\r\n","  for track in tracker.tracks:\r\n","    \r\n","    if not track.is_confirmed() or track.time_since_update > 1:\r\n","      continue \r\n","    bbox = track.to_tlbr()\r\n","    class_name = track.get_class()\r\n","    #print(class_name,track.track_id)\r\n","    if class_name=='person' and track.track_id==1:\r\n","      color = colors[int(track.track_id) % len(colors)]\r\n","      color = [i * 255 for i in color]\r\n","      cv2.rectangle(frame, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\r\n","      cv2.rectangle(frame, (int(bbox[0]), int(bbox[1]-30)), (int(bbox[0])+(len(class_name)+len(str(track.track_id)))*17, int(bbox[1])), color, -1)\r\n","      cv2.putText(frame, 'Employee',(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,255,255),2)\r\n","      present = 1\r\n","      break\r\n","\r\n","  if present==0 and working == 1:\r\n","    out_of_frames+=1\r\n","    if out_of_frames>=tolerance:\r\n","      cv2.putText(frame, 'Email Alert',(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,0,0),2)\r\n","      working = 0\r\n","      ##### EMAIL CODE and TIME ######\r\n","  elif present==1 and working==0:\r\n","    out_of_frames = 0\r\n","    working = 1\r\n","    cv2.putText(frame, 'Returned Back',(int(bbox[0]), int(bbox[1]-10)),0, 0.75, (255,0,0),2)\r\n","    ###### Email Code and TIME\r\n","    \r\n","  text = 'Vigilance '\r\n","  cv2.putText(frame, text, (10, H- ((0 * 20) + 20)),cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\r\n","    # check to see if we should write the frame to disk\r\n","  if writer is not None:\r\n","    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\r\n","    writer.write(frame)\r\n","\r\n","\t# if the `q` key was pressed, break from the loop\r\n","  key = cv2.waitKey(1) & 0xFF\r\n","  if key == ord(\"q\"):\r\n","    print(\"heck\")\r\n","    break\r\n","\r\n","\t# increment the total number of frames processed thus far and\r\n","\t# then update the FPS counter\r\n","  totalFrames += 1\r\n","  if totalFrames>3600:\r\n","    break\r\n","  fps.update()\r\n","\r\n","# stop the timer and display FPS information\r\n","fps.stop()\r\n","\r\n","print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\r\n","print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\r\n","\r\n","# check to see if we need to release the video writer pointer\r\n","if writer is not None:\r\n","\twriter.release()\r\n","\r\n","vs.release()\r\n","\r\n","# close any open windows\r\n","cv2.destroyAllWindows()"],"execution_count":45,"outputs":[{"output_type":"stream","text":["[INFO] opening video file...\n","person\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","person 1\n","hello\n","[INFO] elapsed time: 52.85\n","[INFO] approx. FPS: 14.59\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WaekGRXm_xjv","executionInfo":{"status":"ok","timestamp":1608995658158,"user_tz":-330,"elapsed":34505,"user":{"displayName":"Manohar Sai Alapati220","photoUrl":"","userId":"01265827642573417855"}},"outputId":"f7482568-2282-4b4a-ac4f-98719c0cb434"},"source":["!zip -r /content/Tracking_Files.zip /content/yolov4-deepsort"],"execution_count":43,"outputs":[{"output_type":"stream","text":["updating: content/yolov4-deepsort/ (stored 0%)\n","updating: content/yolov4-deepsort/core/ (stored 0%)\n","updating: content/yolov4-deepsort/core/yolov4.py (deflated 85%)\n","updating: content/yolov4-deepsort/core/__pycache__/ (stored 0%)\n","updating: content/yolov4-deepsort/core/__pycache__/common.cpython-37.pyc (deflated 40%)\n","updating: content/yolov4-deepsort/core/__pycache__/yolov4.cpython-37.pyc (deflated 62%)\n","updating: content/yolov4-deepsort/core/__pycache__/backbone.cpython-36.pyc (deflated 68%)\n","updating: content/yolov4-deepsort/core/__pycache__/backbone.cpython-37.pyc (deflated 66%)\n","updating: content/yolov4-deepsort/core/__pycache__/yolov4.cpython-36.pyc (deflated 64%)\n","updating: content/yolov4-deepsort/core/__pycache__/common.cpython-36.pyc (deflated 41%)\n","updating: content/yolov4-deepsort/core/__pycache__/utils.cpython-37.pyc (deflated 53%)\n","updating: content/yolov4-deepsort/core/__pycache__/utils.cpython-36.pyc (deflated 53%)\n","updating: content/yolov4-deepsort/core/__pycache__/config.cpython-37.pyc (deflated 34%)\n","updating: content/yolov4-deepsort/core/__pycache__/config.cpython-36.pyc (deflated 35%)\n","updating: content/yolov4-deepsort/core/common.py (deflated 65%)\n","updating: content/yolov4-deepsort/core/utils.py (deflated 74%)\n","updating: content/yolov4-deepsort/core/config.py (deflated 62%)\n","updating: content/yolov4-deepsort/core/dataset.py (deflated 80%)\n","updating: content/yolov4-deepsort/core/backbone.py (deflated 90%)\n","updating: content/yolov4-deepsort/save_model.py (deflated 68%)\n","updating: content/yolov4-deepsort/deep_sort/ (stored 0%)\n","updating: content/yolov4-deepsort/deep_sort/__init__.py (stored 0%)\n","updating: content/yolov4-deepsort/deep_sort/iou_matching.py (deflated 66%)\n","updating: content/yolov4-deepsort/deep_sort/linear_assignment.py (deflated 78%)\n","updating: content/yolov4-deepsort/deep_sort/nn_matching.py (deflated 71%)\n","updating: content/yolov4-deepsort/deep_sort/track.py (deflated 71%)\n","updating: content/yolov4-deepsort/deep_sort/detection.py (deflated 65%)\n","updating: content/yolov4-deepsort/deep_sort/tracker.py (deflated 71%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/ (stored 0%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/tracker.cpython-37.pyc (deflated 55%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/linear_assignment.cpython-36.pyc (deflated 65%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/preprocessing.cpython-36.pyc (deflated 45%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/detection.cpython-36.pyc (deflated 54%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/kalman_filter.cpython-36.pyc (deflated 60%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/iou_matching.cpython-36.pyc (deflated 52%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/kalman_filter.cpython-37.pyc (deflated 60%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/nn_matching.cpython-37.pyc (deflated 60%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/tracker.cpython-36.pyc (deflated 55%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/detection.cpython-37.pyc (deflated 54%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/__init__.cpython-36.pyc (deflated 24%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/track.cpython-37.pyc (deflated 60%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/preprocessing.cpython-37.pyc (deflated 45%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/track.cpython-36.pyc (deflated 60%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/linear_assignment.cpython-37.pyc (deflated 65%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/iou_matching.cpython-37.pyc (deflated 51%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/nn_matching.cpython-36.pyc (deflated 61%)\n","updating: content/yolov4-deepsort/deep_sort/__pycache__/__init__.cpython-37.pyc (deflated 22%)\n","updating: content/yolov4-deepsort/deep_sort/kalman_filter.py (deflated 74%)\n","updating: content/yolov4-deepsort/deep_sort/preprocessing.py (deflated 60%)\n","updating: content/yolov4-deepsort/requirements.txt (deflated 10%)\n","updating: content/yolov4-deepsort/conda-gpu.yml (deflated 41%)\n","updating: content/yolov4-deepsort/data/ (stored 0%)\n","updating: content/yolov4-deepsort/data/anchors/ (stored 0%)\n","updating: content/yolov4-deepsort/data/anchors/basline_anchors.txt (deflated 43%)\n","updating: content/yolov4-deepsort/data/anchors/basline_tiny_anchors.txt (deflated 16%)\n","updating: content/yolov4-deepsort/data/anchors/yolov4_anchors.txt (deflated 21%)\n","updating: content/yolov4-deepsort/data/anchors/yolov3_anchors.txt (deflated 24%)\n","updating: content/yolov4-deepsort/data/helpers/ (stored 0%)\n","updating: content/yolov4-deepsort/data/helpers/all_classes.gif (deflated 1%)\n","updating: content/yolov4-deepsort/data/helpers/custom_config.png (deflated 6%)\n","updating: content/yolov4-deepsort/data/helpers/performance.png (deflated 3%)\n","updating: content/yolov4-deepsort/data/helpers/filter_classes.PNG (deflated 6%)\n","updating: content/yolov4-deepsort/data/helpers/custom_result.png (deflated 5%)\n","updating: content/yolov4-deepsort/data/helpers/demo.gif (deflated 1%)\n","updating: content/yolov4-deepsort/data/helpers/cars.gif (deflated 1%)\n","updating: content/yolov4-deepsort/data/dataset/ (stored 0%)\n","updating: content/yolov4-deepsort/data/dataset/val2017.txt (deflated 70%)\n","updating: content/yolov4-deepsort/data/dataset/val2014.txt (deflated 72%)\n","updating: content/yolov4-deepsort/data/video/ (stored 0%)\n","updating: content/yolov4-deepsort/data/video/test.mp4 (deflated 0%)\n","updating: content/yolov4-deepsort/data/video/cars.mp4 (deflated 0%)\n","updating: content/yolov4-deepsort/data/classes/ (stored 0%)\n","updating: content/yolov4-deepsort/data/classes/voc.names (deflated 33%)\n","updating: content/yolov4-deepsort/data/classes/yymnist.names (stored 0%)\n","updating: content/yolov4-deepsort/data/classes/coco.names (deflated 44%)\n","updating: content/yolov4-deepsort/data/yolov4.weights (deflated 7%)\n","updating: content/yolov4-deepsort/requirements-gpu.txt (deflated 11%)\n","updating: content/yolov4-deepsort/tools/ (stored 0%)\n","updating: content/yolov4-deepsort/tools/freeze_model.py (deflated 75%)\n","updating: content/yolov4-deepsort/tools/__pycache__/ (stored 0%)\n","updating: content/yolov4-deepsort/tools/__pycache__/generate_detections.cpython-36.pyc (deflated 48%)\n","updating: content/yolov4-deepsort/tools/generate_detections.py (deflated 66%)\n","updating: content/yolov4-deepsort/README.md (deflated 63%)\n","updating: content/yolov4-deepsort/LICENSE (deflated 41%)\n","updating: content/yolov4-deepsort/checkpoints/ (stored 0%)\n","updating: content/yolov4-deepsort/checkpoints/yolov4-416/ (stored 0%)\n","updating: content/yolov4-deepsort/checkpoints/yolov4-416/saved_model.pb (deflated 92%)\n","updating: content/yolov4-deepsort/checkpoints/yolov4-416/assets/ (stored 0%)\n","updating: content/yolov4-deepsort/checkpoints/yolov4-416/variables/ (stored 0%)\n","updating: content/yolov4-deepsort/checkpoints/yolov4-416/variables/variables.index (deflated 81%)\n","updating: content/yolov4-deepsort/checkpoints/yolov4-416/variables/variables.data-00000-of-00001 (deflated 7%)\n","updating: content/yolov4-deepsort/outputs/ (stored 0%)\n","updating: content/yolov4-deepsort/outputs/demo.avi (deflated 2%)\n","updating: content/yolov4-deepsort/outputs/cars.avi (deflated 1%)\n","updating: content/yolov4-deepsort/convert_trt.py (deflated 66%)\n","updating: content/yolov4-deepsort/object_tracker.py (deflated 66%)\n","updating: content/yolov4-deepsort/.git/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/description (deflated 14%)\n","updating: content/yolov4-deepsort/.git/hooks/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/hooks/post-update.sample (deflated 27%)\n","updating: content/yolov4-deepsort/.git/hooks/pre-commit.sample (deflated 43%)\n","updating: content/yolov4-deepsort/.git/hooks/update.sample (deflated 68%)\n","updating: content/yolov4-deepsort/.git/hooks/pre-rebase.sample (deflated 59%)\n","updating: content/yolov4-deepsort/.git/hooks/fsmonitor-watchman.sample (deflated 53%)\n","updating: content/yolov4-deepsort/.git/hooks/pre-push.sample (deflated 50%)\n","updating: content/yolov4-deepsort/.git/hooks/applypatch-msg.sample (deflated 42%)\n","updating: content/yolov4-deepsort/.git/hooks/commit-msg.sample (deflated 44%)\n","updating: content/yolov4-deepsort/.git/hooks/pre-receive.sample (deflated 40%)\n","updating: content/yolov4-deepsort/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n","updating: content/yolov4-deepsort/.git/hooks/pre-applypatch.sample (deflated 38%)\n","updating: content/yolov4-deepsort/.git/info/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/info/exclude (deflated 28%)\n","updating: content/yolov4-deepsort/.git/logs/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/logs/refs/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/logs/refs/remotes/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/logs/refs/remotes/origin/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/logs/refs/remotes/origin/HEAD (deflated 27%)\n","updating: content/yolov4-deepsort/.git/logs/refs/heads/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/logs/refs/heads/master (deflated 27%)\n","updating: content/yolov4-deepsort/.git/logs/HEAD (deflated 27%)\n","updating: content/yolov4-deepsort/.git/objects/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/objects/info/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/objects/pack/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/objects/pack/pack-307fe0079d96c54a34985c3ee8be28df44df1d3f.pack (deflated 0%)\n","updating: content/yolov4-deepsort/.git/objects/pack/pack-307fe0079d96c54a34985c3ee8be28df44df1d3f.idx (deflated 16%)\n","updating: content/yolov4-deepsort/.git/index (deflated 51%)\n","updating: content/yolov4-deepsort/.git/branches/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/refs/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/refs/remotes/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/refs/remotes/origin/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/refs/remotes/origin/HEAD (stored 0%)\n","updating: content/yolov4-deepsort/.git/refs/heads/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/refs/heads/master (stored 0%)\n","updating: content/yolov4-deepsort/.git/refs/tags/ (stored 0%)\n","updating: content/yolov4-deepsort/.git/config (deflated 31%)\n","updating: content/yolov4-deepsort/.git/packed-refs (deflated 10%)\n","updating: content/yolov4-deepsort/.git/HEAD (stored 0%)\n","updating: content/yolov4-deepsort/model_data/ (stored 0%)\n","updating: content/yolov4-deepsort/model_data/mars-small128.pb (deflated 8%)\n","updating: content/yolov4-deepsort/convert_tflite.py (deflated 62%)\n","updating: content/yolov4-deepsort/conda-cpu.yml (deflated 40%)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SE7hzvVOnNKX"},"source":[""],"execution_count":null,"outputs":[]}]}